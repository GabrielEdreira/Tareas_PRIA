{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a04d08b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2dc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702b609",
   "metadata": {},
   "source": [
    "# A) Viajes Nocturnos (00:00-01:00) por Día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91988ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A():\n",
    "    # Celda A1: Spark SQL\n",
    "    print(\"A1) Spark SQL - Viajes nocturnos por día:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DATE(tpep_pickup_datetime) AS dia,\n",
    "            COUNT(*) AS num_viajes\n",
    "        FROM taxi_data\n",
    "        WHERE HOUR(tpep_pickup_datetime) = 0\n",
    "        GROUP BY dia\n",
    "        ORDER BY dia\n",
    "    \"\"\").show(30, truncate=False)\n",
    "\n",
    "    # Celda A2: API DataFrame\n",
    "    print(\"\\nA2) API DataFrame - Viajes nocturnos por día:\")\n",
    "    (df_clean\n",
    "     .filter(hour(\"tpep_pickup_datetime\") == 0)\n",
    "     .groupBy(to_date(\"tpep_pickup_datetime\").alias(\"dia\"))\n",
    "     .count()\n",
    "     .orderBy(\"dia\")\n",
    "     .show(30, truncate=False))\n",
    "\n",
    "    # Celda A3: Notación estilo RDD\n",
    "    print(\"A3) Spark RDD - Viajes nocturnos por día:\")\n",
    "    nocturnos_por_dia = (df_rdd.rdd\n",
    "                         .filter(lambda x: x.tpep_pickup_datetime.hour == 0)\n",
    "                         .map(lambda x: (x.tpep_pickup_datetime.date(), 1))\n",
    "                         .reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[0])\n",
    "                        )\n",
    "\n",
    "    for dia, num in nocturnos_por_dia.collect()[:30]:\n",
    "        print(str(dia) + \"\\t\" + str(num))\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b8495",
   "metadata": {},
   "source": [
    "# B) Viajes Nocturnos (00:00-01:00) por Mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B():\n",
    "    # Celda B1: Spark SQL\n",
    "    print(\"B1) Spark SQL - Viajes nocturnos por mes:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            MONTH(tpep_pickup_datetime) AS mes,\n",
    "            COUNT(*) AS num_viajes\n",
    "        FROM taxi_data\n",
    "        WHERE HOUR(tpep_pickup_datetime) = 0\n",
    "        GROUP BY mes\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda B2: API DataFrame\n",
    "\n",
    "    print(\"\\nB2) API DataFrame - Viajes nocturnos por mes:\")\n",
    "    (df_clean\n",
    "     .filter(hour(\"tpep_pickup_datetime\") == 0)\n",
    "     .groupBy(month(\"tpep_pickup_datetime\").alias(\"mes\"))\n",
    "     .count()\n",
    "     .show())\n",
    "\n",
    "    # Celda B3: Viajes nocturnos por mes (RDD)\n",
    "    print(\"B3) Spark RDD - Viajes nocturnos por mes:\")\n",
    "\n",
    "    resultados = (df_rdd.rdd\n",
    "        .filter(lambda r: r.tpep_pickup_datetime.hour == 0)\n",
    "        .map(lambda r: (r.tpep_pickup_datetime.month, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .sortBy(lambda x: x[0])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    print(\"Mes\\tTotal Viajes\")\n",
    "    for mes, total in resultados:\n",
    "        print(\"%d\\t%d\" % (mes, total))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf85d1",
   "metadata": {},
   "source": [
    "# C) Media de Viajes por Mes por Conductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def C():\n",
    "    # Celda C1: Spark SQL\n",
    "    print(\"C1) Spark SQL - Media viajes/mes por conductor:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            VendorID,\n",
    "            AVG(viajes_por_mes) AS media_viajes_mes\n",
    "        FROM (\n",
    "            SELECT \n",
    "                VendorID, \n",
    "                MONTH(tpep_pickup_datetime) AS mes,\n",
    "                COUNT(*) AS viajes_por_mes\n",
    "            FROM taxi_data\n",
    "            GROUP BY VendorID, mes\n",
    "        )\n",
    "        GROUP BY VendorID\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda C2: API DataFrame\n",
    "    print(\"\\nC2) API DataFrame - Media viajes/mes por conductor:\")\n",
    "    (df_clean\n",
    "     .groupBy(\"VendorID\", month(\"tpep_pickup_datetime\").alias(\"mes\"))\n",
    "     .count()\n",
    "     .groupBy(\"VendorID\")\n",
    "     .agg(avg(\"count\").alias(\"media_viajes_mes\"))\n",
    "     .show())\n",
    "\n",
    "    # Celda C3: Media viajes/mes por conductor (RDD)\n",
    "    print(\"C3) Spark RDD - Media viajes/mes por conductor:\")\n",
    "\n",
    "    viajes_mes_conductor = (df_rdd.rdd\n",
    "        .map(lambda r: ((r.VendorID, r.tpep_pickup_datetime.month), 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    media_viajes = (viajes_mes_conductor\n",
    "        .map(lambda x: (x[0][0], (x[1], 1)))\n",
    "        .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "        .mapValues(lambda x: x[0] / float(x[1]))\n",
    "        .sortBy(lambda x: x[0])\n",
    "    )\n",
    "\n",
    "    print(\"VendorID | Media Viajes/Mes\")\n",
    "    for vendor_id, media in media_viajes.collect():\n",
    "        print(str(vendor_id) + \" |  \" + str(media))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8ca8c",
   "metadata": {},
   "source": [
    "# D) Media de Viajes por Día por Conductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12879ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D():\n",
    "    # Celda D1: Spark SQL\n",
    "    print(\"D1) Spark SQL - Media viajes/día por conductor:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            VendorID,\n",
    "            AVG(viajes_por_dia) AS media_viajes_dia\n",
    "        FROM (\n",
    "            SELECT \n",
    "                VendorID, \n",
    "                DATE(tpep_pickup_datetime) AS dia,\n",
    "                COUNT(*) AS viajes_por_dia\n",
    "            FROM taxi_data\n",
    "            GROUP BY VendorID, dia\n",
    "        )\n",
    "        GROUP BY VendorID\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda D2: API DataFrame\n",
    "    print(\"\\nD2) API DataFrame - Media viajes/día por conductor:\")\n",
    "    (df_clean\n",
    "     .groupBy(\"VendorID\", to_date(\"tpep_pickup_datetime\").alias(\"dia\"))\n",
    "     .count()\n",
    "     .groupBy(\"VendorID\")\n",
    "     .agg(avg(\"count\").alias(\"media_viajes_dia\"))\n",
    "     .show())\n",
    "\n",
    "    # Celda D3: Spark RDD - Media viajes/día por conductor\n",
    "    print(\"D3) Spark RDD - Media viajes/día por conductor:\")\n",
    "\n",
    "    resultados = (df_rdd.rdd\n",
    "        .map(lambda r: ((r.VendorID, r.tpep_pickup_datetime.date()), 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .map(lambda x: (x[0][0], (x[1], 1)))\n",
    "        .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "        .mapValues(lambda x: x[0]/float(x[1]))\n",
    "        .sortBy(lambda x: x[0])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    print(\"VendorID | Media Viajes/Día\")\n",
    "    for vendor_id, media in resultados:\n",
    "        print(str(vendor_id) + \" | \" + str(media))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db981cb1",
   "metadata": {},
   "source": [
    "# E) Máximo Pasajeros en Primera Semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8457b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E():\n",
    "    # Celda E1: Spark SQL\n",
    "    print(\"E1) Spark SQL - Máx pasajeros en primera semana:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT MAX(passenger_count) AS max_pasajeros\n",
    "        FROM taxi_data\n",
    "        WHERE DAY(tpep_pickup_datetime) <= 7\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda E2: API DataFrame\n",
    "    print(\"\\nE2) API DataFrame - Máx pasajeros en primera semana:\")\n",
    "    (df_clean\n",
    "     .filter(dayofmonth(\"tpep_pickup_datetime\") <= 7)\n",
    "     .agg(max(\"passenger_count\").alias(\"max_pasajeros\"))\n",
    "     .show())\n",
    "\n",
    "    # Celda E3: Spark RDD - Máx pasajeros en primera semana\n",
    "    print(\"E3) Spark RDD - Máx pasajeros en primera semana:\")\n",
    "\n",
    "    max_pasajeros = (df_rdd.rdd\n",
    "        .filter(lambda r: r.tpep_pickup_datetime.day <= 7)\n",
    "        .map(lambda r: r.passenger_count)\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    print(\"Máximo de pasajeros: \" + str(max_pasajeros))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b7030",
   "metadata": {},
   "source": [
    "# F) Máximo Pasajeros en Todo el Mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F():\n",
    "    # Celda F1: Spark SQL\n",
    "    print(\"F1) Spark SQL - Máx pasajeros en todo el mes:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT MAX(passenger_count) AS max_pasajeros_mes\n",
    "        FROM taxi_data\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda F2: API DataFrame\n",
    "    print(\"\\nF2) API DataFrame - Máx pasajeros en todo el mes:\")\n",
    "    df_clean.agg(max(\"passenger_count\").alias(\"max_pasajeros_mes\")).show()\n",
    "\n",
    "    # Celda F3: Spark RDD - Máx pasajeros en todo el mes\n",
    "    print(\"F3) Spark RDD - Max pasajeros en todo el mes:\")\n",
    "\n",
    "    max_pasajeros = (df_rdd.rdd\n",
    "        .map(lambda r: r.passenger_count)\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    print(\"Maximo de pasajeros: \" + str(max_pasajeros))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942e8de",
   "metadata": {},
   "source": [
    "# G) Coste del Recorrido más Caro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G():\n",
    "    # Celda G1: Spark SQL\n",
    "    print(\"G1) Spark SQL - Recorrido más caro:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT MAX(total_amount) AS max_coste\n",
    "        FROM taxi_data\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda G2: API DataFrame\n",
    "    print(\"\\nG2) API DataFrame - Recorrido más caro:\")\n",
    "    df_clean.agg(max(\"total_amount\").alias(\"max_coste\")).show()\n",
    "\n",
    "    # Celda G3: Spark RDD - Recorrido más caro\n",
    "    print(\"G3) Spark RDD - Recorrido mas caro:\")\n",
    "\n",
    "    max_coste = (df_rdd.rdd\n",
    "        .map(lambda r: float(r.total_amount))\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    print(\"Costo máximo: \" + str(max_coste))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d581d",
   "metadata": {},
   "source": [
    "# H) Coste del Recorrido más Barato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H():\n",
    "    # Celda H1: Spark SQL\n",
    "    print(\"H1) Spark SQL - Recorrido más barato:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT MIN(total_amount) AS min_coste\n",
    "        FROM taxi_data\n",
    "        WHERE total_amount > 0\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda H2: API DataFrame\n",
    "    print(\"\\nH2) API DataFrame - Recorrido más barato:\")\n",
    "    df_clean.filter(\"total_amount > 0\").agg(min(\"total_amount\").alias(\"min_coste\")).show()\n",
    "\n",
    "    # Celda H3: Spark RDD - Recorrido más barato\n",
    "    print(\"H3) Spark RDD - Recorrido mas barato:\")\n",
    "\n",
    "    min_coste = (df_rdd.rdd \n",
    "        .filter(lambda r: float(r.total_amount) > 0)\n",
    "        .map(lambda r: float(r.total_amount))\n",
    "        .min()\n",
    "    )\n",
    "\n",
    "    print(\"Costo minimo: \" + str(min_coste))\n",
    "    print()def H():\n",
    "    # Celda H1: Spark SQL\n",
    "    print(\"H1) Spark SQL - Recorrido más barato:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT MIN(total_amount) AS min_coste\n",
    "        FROM taxi_data\n",
    "        WHERE total_amount > 0\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Celda H2: API DataFrame\n",
    "    print(\"\\nH2) API DataFrame - Recorrido más barato:\")\n",
    "    df_clean.filter(\"total_amount > 0\").agg(min(\"total_amount\").alias(\"min_coste\")).show()\n",
    "\n",
    "    # Celda H3: Spark RDD - Recorrido más barato\n",
    "    print(\"H3) Spark RDD - Recorrido mas barato:\")\n",
    "\n",
    "    min_coste = (df_rdd.rdd \n",
    "        .filter(lambda r: float(r.total_amount) > 0)\n",
    "        .map(lambda r: float(r.total_amount))\n",
    "        .min()\n",
    "    )\n",
    "\n",
    "    print(\"Costo minimo: \" + str(min_coste))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd026f8",
   "metadata": {},
   "source": [
    "# Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fraction = 0.2\n",
    "format_string = \"yyyy-MM-dd HH:mm:ss\"\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Cargar y limpiar datos\n",
    "file_path = \"yellow_tripdata_2018-11.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Limpieza básica\n",
    "df_clean = df.dropna(subset=[\"tpep_pickup_datetime\", \"VendorID\", \"passenger_count\", \"total_amount\"])\n",
    "df_clean = df_clean.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "df_clean = df_clean.withColumn(\"tpep_pickup_datetime\", \n",
    "                             to_timestamp(\"tpep_pickup_datetime\", format_string))\n",
    "\n",
    "df_clean.createOrReplaceTempView(\"taxi_data\")\n",
    "\n",
    "# Convertir a Pandas para el enfoque 3\n",
    "df_rdd = df_clean\n",
    "\n",
    "# Ejecutamos los métodos\n",
    "A()\n",
    "B()\n",
    "C()\n",
    "D()\n",
    "E()\n",
    "F()\n",
    "G()\n",
    "H()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
