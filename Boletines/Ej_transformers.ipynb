{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frKiJg8auD2D",
        "outputId": "18c73e40-7ac0-4056-ffa4-85b6c59c78ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from transformers) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-20.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.11.18-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.6.0-cp313-cp313-win_amd64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading multidict-6.4.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.20.0-cp313-cp313-win_amd64.whl.metadata (74 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\gabriel.antonio.edre\\documents\\main\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
            "   ----------------------------- ---------- 7.6/10.4 MB 40.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.4/10.4 MB 34.8 MB/s eta 0:00:00\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
            "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading pyarrow-20.0.0-cp313-cp313-win_amd64.whl (25.7 MB)\n",
            "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
            "   ------------------- -------------------- 12.3/25.7 MB 60.6 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 21.0/25.7 MB 48.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.4/25.7 MB 45.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.7/25.7 MB 38.9 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.4/2.4 MB 27.9 MB/s eta 0:00:00\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
            "Downloading aiohttp-3.11.18-cp313-cp313-win_amd64.whl (437 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.6.0-cp313-cp313-win_amd64.whl (119 kB)\n",
            "Downloading multidict-6.4.3-cp313-cp313-win_amd64.whl (38 kB)\n",
            "Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl (44 kB)\n",
            "Downloading yarl-1.20.0-cp313-cp313-win_amd64.whl (92 kB)\n",
            "Installing collected packages: xxhash, safetensors, regex, pyarrow, propcache, multidict, fsspec, frozenlist, filelock, dill, attrs, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 huggingface-hub-0.31.2 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3 xxhash-3.5.0 yarl-1.20.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTWHxKbujIw"
      },
      "source": [
        "El archivo .json tendr치 el formato instruction, response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY-SjQMjuKkZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabriel.antonio.edre\\Documents\\main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/ejemplo.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#El collator DataCollatorForLanguageModeling lo utilizamos para modelos como Qwen, que aprenden de forma autoregresiva\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Cargar dataset desde archivo JSON\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/sample_data/ejemplo.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(data)\n",
            "File \u001b[1;32mc:\\Users\\gabriel.antonio.edre\\Documents\\main\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/ejemplo.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,Trainer,TrainingArguments,DataCollatorForLanguageModeling\n",
        "#El collator DataCollatorForLanguageModeling lo utilizamos para modelos como Qwen, que aprenden de forma autoregresiva\n",
        "\n",
        "\n",
        "# 1. Cargar dataset desde archivo JSON\n",
        "# dataset_path = \"/content/sample_data/ejemplo.json\"\n",
        "# with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     data = json.load(f)\n",
        "# dataset = Dataset.from_list(data)\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"naklecha/minecraft-question-answer-700k\")\n",
        "\n",
        "\n",
        "# 2. Cargar modelo y tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 3. Asegurar que haya token de padding\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 4. Agregar tokens especiales si no est치n\n",
        "special_tokens = ['<|user|>', '<|assistant|>']\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 5. Construcci칩n del prompt\n",
        "def build_prompt(example):\n",
        "    prompt = f\"<|user|>\\n{example['instruction']}\\n<|assistant|>\\n{example['response']}\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = dataset.map(build_prompt, remove_columns=[\"instruction\", \"response\"])\n",
        "\n",
        "# 6. Tokenizaci칩n\n",
        "def tokenize(example):\n",
        "    out = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "    return out\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# 7. Dividir en train/test\n",
        "splits = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_data = splits[\"train\"]\n",
        "eval_data = splits[\"test\"]\n",
        "\n",
        "# 8. Collator para entrenamiento causal\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Muy importante: no es masked LM, es causal LM\n",
        ")\n",
        "\n",
        "# 9. Configurar entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"  # Desactiva W&B y otros trackers\n",
        ")\n",
        "\n",
        "# 10. Inicializar y entrenar\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    tokenizer=tokenizer, #processing_class=\"tokenizer\"\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 11. Guardar modelo fine-tuneado\n",
        "trainer.save_model(\"./results/Qwen_finetuned\")\n",
        "tokenizer.save_pretrained(\"./results/Qwen_finetuned\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
